# meta_crew/templates/crew_exec_template.py

import os
import sys
import datetime
import json
import re
from dotenv import load_dotenv
from langchain_anthropic import ChatAnthropic
from crewai import Agent, Task, Crew, Process

# Charger les variables d'environnement depuis le fichier .env √† la racine du projet
root_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
dotenv_path = os.path.join(root_dir, ".env")
load_dotenv(dotenv_path)

class ArtifactManager:
    """Gestionnaire d'artefacts pour stocker les codes et documents complets."""
    
    def __init__(self, project_name):
        """Initialise le gestionnaire d'artefacts avec un nom de projet."""
        self.project_dir = os.path.dirname(os.path.abspath(__file__))
        self.artifacts_dir = os.path.join(self.project_dir, "artifacts")
        self.code_dir = os.path.join(self.artifacts_dir, "code")
        self.docs_dir = os.path.join(self.artifacts_dir, "docs")
        self.tests_dir = os.path.join(self.artifacts_dir, "tests")
        
        # Cr√©ation des r√©pertoires n√©cessaires
        for directory in [self.artifacts_dir, self.code_dir, self.docs_dir, self.tests_dir]:
            os.makedirs(directory, exist_ok=True)
        
        self.project_name = project_name
        
    def save_code(self, filename, content, language="python"):
        """Sauvegarde un fichier de code complet."""
        filepath = os.path.join(self.code_dir, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"‚úÖ Code sauvegard√© dans {filepath}")
        return filepath
        
    def save_document(self, filename, content):
        """Sauvegarde un document."""
        filepath = os.path.join(self.docs_dir, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"üìÑ Document sauvegard√© dans {filepath}")
        return filepath
        
    def save_test(self, filename, content):
        """Sauvegarde un fichier de test."""
        filepath = os.path.join(self.tests_dir, filename)
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"üß™ Test sauvegard√© dans {filepath}")
        return filepath
        
    def list_artifacts(self):
        """Liste tous les artefacts cr√©√©s."""
        artifacts = []
        for root, _, files in os.walk(self.artifacts_dir):
            for file in files:
                rel_path = os.path.relpath(os.path.join(root, file), self.artifacts_dir)
                artifacts.append(rel_path)
        return artifacts

def extract_code_from_response(response_text):
    """
    Extrait le code source des r√©ponses des agents.
    
    Args:
        response_text (str): Texte complet de la r√©ponse
    
    Returns:
        dict: Dictionnaire des fichiers avec leur code source
    """
    code_files = {}
    
    # Recherche de blocs de code avec indication de langage
    code_blocks = re.finditer(r'```(?:python|javascript|java|html|css|c\+\+|php|)?\s*(.*?)```', response_text, re.DOTALL)
    
    for i, match in enumerate(code_blocks):
        code = match.group(1).strip()
        
        # Rechercher un nom de fichier potentiel avant le bloc de code
        file_name_match = re.search(r'(?:fichier|file):\s*[\'"]?([a-zA-Z0-9_\-\.]+\.[a-zA-Z0-9]+)[\'"]?', 
                                   response_text[:match.start()], re.IGNORECASE)
        
        if file_name_match:
            filename = file_name_match.group(1)
        else:
            # Essayer de d√©duire le nom du fichier √† partir du contenu du code
            if "class Character" in code and ".py" not in code:
                filename = "character.py"
            elif "def generate" in code and ".py" not in code:
                filename = "generator.py"
            elif "import React" in code:
                filename = "app.jsx"
            elif "<html" in code:
                filename = "index.html"
            elif "body {" in code:
                filename = "style.css"
            else:
                filename = f"code_block_{i+1}.py"
        
        code_files[filename] = code
    
    return code_files

def extract_test_report(response_text):
    """
    Extrait le rapport de test des r√©ponses des agents.
    
    Args:
        response_text (str): Texte complet de la r√©ponse
    
    Returns:
        str: Rapport de test format√© en markdown
    """
    # Recherche d'un rapport de test
    test_report_match = re.search(r'Rapport de test.*?(?=```|$)', response_text, re.DOTALL | re.IGNORECASE)
    
    if test_report_match:
        return f"# Rapport de test\n\n{test_report_match.group(0).strip()}"
    
    return None

def exec_crew():
    """
    Ex√©cute une crew pour le d√©veloppement du projet.
    Cette fonction inclut une boucle d'auto-√©valuation qui permet jusqu'√† 2 it√©rations.
    
    Returns:
        dict: R√©sultats de l'ex√©cution
    """
    # V√©rifier que la cl√© API est disponible
    if "ANTHROPIC_API_KEY" not in os.environ:
        print("Erreur: ANTHROPIC_API_KEY n'est pas d√©finie dans les variables d'environnement.")
        print(f"Chemin du .env recherch√©: {dotenv_path}")
        sys.exit(1)
    
    # Initialisation du compteur d'it√©rations
    iteration_count = 0
    max_iterations = 2
    approved = False
    
    # Nom du projet
    project_name = os.path.basename(os.path.dirname(os.path.abspath(__file__)))
    
    # Cr√©ation du gestionnaire d'artefacts
    artifact_manager = ArtifactManager(project_name)
    
    # Cr√©ation du r√©pertoire logs s'il n'existe pas
    logs_dir = "logs"
    os.makedirs(logs_dir, exist_ok=True)
    
    # Liste pour stocker les r√©sultats des diff√©rentes it√©rations
    all_results = []
    
    # Initialisation du mod√®le LLM avec la cl√© API depuis les variables d'environnement
    llm = ChatAnthropic(
        model="claude-3-opus-20240229", 
        temperature=0.2,
        max_tokens_to_sample=4000,
        anthropic_api_key=os.environ["ANTHROPIC_API_KEY"]
    )
    
    while iteration_count < max_iterations and not approved:
        iteration_count += 1
        print(f"D√©marrage de l'it√©ration {iteration_count}/{max_iterations}")
        
        # Cr√©ation des agents avec des instructions am√©lior√©es
        project_manager = Agent(
            role="Project Manager",
            goal=f"Superviser et coordonner l'ensemble du projet {project_name}",
            backstory="Un chef de projet exp√©riment√© avec une expertise dans la gestion de projets complexes.",
            llm=llm,
            verbose=True
        )
        
        developer = Agent(
            role="D√©veloppeur",
            goal=f"Impl√©menter des fichiers complets et fonctionnels pour le projet {project_name}",
            backstory=(
                "Un d√©veloppeur exp√©riment√© avec une expertise en architecture logicielle. "
                "Votre mission est de produire du code complet, fonctionnel et bien document√©. "
                "Pour chaque fichier que vous cr√©ez, assurez-vous qu'il est complet du d√©but √† la fin, "
                "incluant toutes les importations, classes, m√©thodes et la documentation n√©cessaire."
            ),
            llm=llm,
            verbose=True
        )
        
        tester = Agent(
            role="Testeur",
            goal=f"Assurer la qualit√© et la fiabilit√© du projet {project_name}",
            backstory=(
                "Expert en assurance qualit√© qui cherche √† garantir l'excellence des logiciels. "
                "Votre mission est de tester de mani√®re exhaustive toutes les fonctionnalit√©s, "
                "de documenter les cas de test, les r√©sultats et les recommandations d'am√©lioration "
                "dans un format clair et complet. Incluez des exemples concrets avec les entr√©es et sorties attendues."
            ),
            llm=llm,
            verbose=True
        )
        
        quality_controller = Agent(
            role="QualityController",
            goal=f"√âvaluer la qualit√© globale du projet {project_name} et ses livrables",
            backstory=(
                "Un expert en contr√¥le qualit√© avec une exp√©rience approfondie dans l'√©valuation de projets similaires. "
                "Votre mission est d'examiner de mani√®re critique tous les livrables, d'identifier les probl√®mes, "
                "les non-conformit√©s et de proposer des am√©liorations concr√®tes."
            ),
            llm=llm,
            verbose=True
        )
        
        # Instructions pour le d√©veloppeur afin de cr√©er des fichiers complets
        dev_instructions = (
            f"Impl√©mentez les fonctionnalit√©s de base du projet {project_name}. "
            "Il est CRUCIAL que vous fournissiez du code COMPLET, du d√©but √† la fin, "
            "sans aucune troncature ou section manquante. Votre r√©ponse doit inclure :\n\n"
            "1. Une analyse des besoins et une description de l'architecture choisie\n"
            "2. Le code source COMPLET pour CHAQUE fichier n√©cessaire, en pr√©cisant clairement "
            "le nom de chaque fichier avant son contenu\n"
            "3. Une explication de l'impl√©mentation et des instructions d'utilisation\n\n"
            "IMPORTANT : Assurez-vous que chaque fichier de code contient toutes les importations, "
            "toutes les d√©finitions de classes/fonctions, et est pr√™t √† √™tre ex√©cut√© sans modification. "
            "Ne tronquez JAMAIS le code avec des commentaires comme '# reste du code...' "
            "ou des ellipses '...'."
        )
        
        # Instructions pour le testeur afin de cr√©er des rapports de test complets
        test_instructions = (
            f"Testez les fonctionnalit√©s impl√©ment√©es du projet {project_name}. "
            "Il est CRUCIAL que vous fournissiez un rapport de test COMPLET et D√âTAILL√â. "
            "Votre r√©ponse doit inclure :\n\n"
            "1. Une description d√©taill√©e de la m√©thodologie de test\n"
            "2. Des cas de test sp√©cifiques avec entr√©es et sorties attendues\n"
            "3. Les r√©sultats obtenus pour CHAQUE test, avec des exemples concrets\n"
            "4. Une analyse des performances et des limites identifi√©es\n"
            "5. Des recommandations d'am√©lioration sp√©cifiques\n\n"
            "IMPORTANT : Incluez dans votre rapport au moins 3-5 exemples complets "
            "de personnages g√©n√©r√©s avec toutes leurs caract√©ristiques."
        )
        
        # Cr√©ation des t√¢ches de d√©veloppement et de test avec les instructions am√©lior√©es
        dev_task = Task(
            description=dev_instructions,
            expected_output="Code source complet et fonctionnel pour tous les fichiers du projet",
            agent=developer
        )
        
        test_task = Task(
            description=test_instructions,
            expected_output="Rapport de test complet avec exemples et recommandations",
            agent=tester
        )
        
        try:
            # Ex√©cuter les t√¢ches de d√©veloppement et de test
            dev_test_crew = Crew(
                agents=[developer, tester],
                tasks=[dev_task, test_task],
                process=Process.sequential,
                verbose=True
            )
            
            dev_test_results = dev_test_crew.kickoff()
            
            # Stocker les r√©sultats
            all_results.append({
                "iteration": iteration_count,
                "dev_results": str(dev_test_results),
                "tasks": [dev_task, test_task]
            })
            
            # Extraction et sauvegarde du code depuis les r√©sultats
            code_results = extract_code_from_response(str(dev_test_results))
            for filename, code in code_results.items():
                artifact_manager.save_code(filename, code)
            
            # Extraction et sauvegarde des r√©sultats de test
            test_report = extract_test_report(str(dev_test_results))
            if test_report:
                artifact_manager.save_document("test_report.md", test_report)
            
            # √âcrire les r√©sultats dans un fichier pour r√©f√©rence
            with open(os.path.join(logs_dir, f"iteration_{iteration_count}_results.txt"), "w", encoding="utf-8") as f:
                f.write(str(dev_test_results))
            
            # Si c'est la derni√®re it√©ration, on n'a pas besoin d'√©valuation
            if iteration_count >= max_iterations:
                # Si c'est la derni√®re it√©ration, on consid√®re que c'est approuv√©
                approved = True
                continue
            
            # Pr√©parer la description de la t√¢che d'auto-√©valuation avec les r√©sultats pr√©c√©dents
            review_description = (
                f"Examiner les livrables de l'it√©ration {iteration_count} du projet {project_name} "
                f"et √©valuer si le projet r√©pond aux exigences. Votre rapport doit √™tre COMPLET et D√âTAILL√â. "
                f"Analysez la qualit√© du code, la couverture des tests et la fonctionnalit√© globale. "
                f"S'il y a des probl√®mes majeurs, incluez le mot 'retry' dans votre r√©ponse. "
                f"Si le projet est approuv√©, expliquez clairement pourquoi.\n\n"
                f"Voici les artefacts produits lors de cette it√©ration :\n"
                f"{json.dumps(artifact_manager.list_artifacts(), indent=2)}\n\n"
                f"Voici un aper√ßu des r√©sultats de l'it√©ration actuelle :\n"
                f"{str(dev_test_results)[:2000]}..."
            )
            
            # Cr√©er la t√¢che d'auto-√©valuation
            review_task = Task(
                description=review_description,
                expected_output="Rapport d'√©valuation d√©taill√© avec d√©cision finale: approuv√© ou retry",
                agent=quality_controller
            )
            
            # Cr√©er une crew s√©par√©e pour l'auto-√©valuation
            review_crew = Crew(
                agents=[quality_controller],
                tasks=[review_task],
                process=Process.sequential,
                verbose=True
            )
            
            try:
                # Ex√©cuter l'auto-√©valuation
                review_results = review_crew.kickoff()
                
                # Stocker les r√©sultats
                all_results[-1]["review_results"] = str(review_results)
                all_results[-1]["tasks"].append(review_task)
                
                # Sauvegarde du rapport d'√©valuation
                quality_report = str(review_results)
                artifact_manager.save_document(f"quality_report_iteration_{iteration_count}.md", quality_report)
                
                # √âcrire les r√©sultats dans un fichier pour r√©f√©rence
                with open(os.path.join(logs_dir, f"iteration_{iteration_count}_review.txt"), "w", encoding="utf-8") as f:
                    f.write(str(review_results))
                
                # Analyser le r√©sultat pour d√©cider si une autre it√©ration est n√©cessaire
                review_result_str = str(review_results)
                if "retry" not in review_result_str.lower():
                    approved = True
                    print(f"Auto-√©valuation approuv√©e √† l'it√©ration {iteration_count}")
                else:
                    print(f"Auto-√©valuation recommande une nouvelle it√©ration")
                    
                # Si c'est la derni√®re it√©ration, indiquer que la limite est atteinte
                    if iteration_count >= max_iterations:
                        print(f"Nombre maximum d'it√©rations atteint ({max_iterations})")
                
            except Exception as review_error:
                print(f"Erreur lors de l'ex√©cution de la t√¢che d'auto-√©valuation: {str(review_error)}")
                # Enregistrer l'erreur dans un fichier de log
                error_log_path = os.path.join(logs_dir, "error.log")
                with open(error_log_path, "a", encoding="utf-8") as error_log:
                    error_log.write(f"=== ERREUR D'EX√âCUTION DE L'AUTO-√âVALUATION - {datetime.datetime.now()} ===\n")
                    error_log.write(f"It√©ration: {iteration_count}\n")
                    error_log.write(f"Exception: {str(review_error)}\n\n")
                
                # Si c'est la derni√®re it√©ration, sortir de la boucle
                if iteration_count >= max_iterations:
                    print(f"Nombre maximum d'it√©rations atteint ({max_iterations})")
                    
        except Exception as e:
            print(f"Erreur lors de l'ex√©cution de la crew: {str(e)}")
            # Enregistrer l'erreur dans un fichier de log
            error_log_path = os.path.join(logs_dir, "error.log")
            with open(error_log_path, "a", encoding="utf-8") as error_log:
                error_log.write(f"=== ERREUR D'EX√âCUTION - {datetime.datetime.now()} ===\n")
                error_log.write(f"It√©ration: {iteration_count}\n")
                error_log.write(f"Exception: {str(e)}\n\n")
            
            # Si c'est la derni√®re it√©ration, sortir de la boucle
            if iteration_count >= max_iterations:
                print(f"Nombre maximum d'it√©rations atteint ({max_iterations})")
    
    # G√©n√©rer le fichier summary.md
    generate_summary(all_results, artifact_manager, iteration_count, [developer, tester, quality_controller], approved)
    
    return all_results

def generate_summary(results, artifact_manager, iterations, agents, approved):
    """
    G√©n√®re un fichier summary.md avec un r√©capitulatif des r√©sultats
    
    Args:
        results (list): Liste des r√©sultats des it√©rations
        artifact_manager (ArtifactManager): Gestionnaire d'artefacts
        iterations (int): Nombre d'it√©rations effectu√©es
        agents (list): Liste des agents utilis√©s
        approved (bool): Indique si le projet a √©t√© approuv√©
    """
    project_name = artifact_manager.project_name
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # Liste des artefacts g√©n√©r√©s
    artifacts = artifact_manager.list_artifacts()
    
    with open(os.path.join(artifact_manager.project_dir, "summary.md"), "w", encoding="utf-8") as f:
        f.write(f"# R√©sum√© du projet {project_name}\n\n")
        f.write(f"Date: {timestamp}\n\n")
        f.write(f"## Informations g√©n√©rales\n\n")
        f.write(f"- Nombre d'it√©rations: {iterations}\n")
        f.write(f"- Statut final: {'Approuv√©' if approved else 'N√©cessite des am√©liorations'}\n\n")
        
        f.write(f"## Agents utilis√©s\n\n")
        for agent in agents:
            f.write(f"### {agent.role}\n")
            f.write(f"- Objectif: {agent.goal}\n")
            f.write(f"- Exp√©rience: {agent.backstory}\n\n")
        
        f.write(f"## Artefacts produits\n\n")
        f.write("| Fichier | Type |\n")
        f.write("| ------ | ---- |\n")
        for artifact in artifacts:
            artifact_type = "Code" if artifact.startswith("code/") else "Document" if artifact.startswith("docs/") else "Test"
            f.write(f"| {artifact} | {artifact_type} |\n")
        f.write("\n\n")
        
        f.write(f"## T√¢ches accomplies par it√©ration\n\n")
        
        for iteration_data in results:
            iteration_num = iteration_data.get("iteration", "?")
            tasks = iteration_data.get("tasks", [])
            
            f.write(f"### It√©ration {iteration_num}\n\n")
            
            for task in tasks:
                short_description = task.description.split('\n')[0]
                f.write(f"#### {short_description}\n")
                f.write(f"- Agent responsable: {task.agent.role}\n")
                f.write(f"- R√©sultat attendu: {task.expected_output}\n\n")
        
        f.write(f"## Conclusion\n\n")
        if iterations == 1 and approved:
            f.write("Le projet a √©t√© approuv√© d√®s la premi√®re it√©ration, ce qui t√©moigne d'une excellente qualit√© des livrables.\n")
        elif approved:
            f.write(f"Le projet a n√©cessit√© {iterations} it√©rations avant d'√™tre approuv√©. Des am√©liorations ont √©t√© apport√©es suite aux retours du contr√¥leur qualit√©.\n")
        else:
            f.write(f"Le projet a atteint le nombre maximum d'it√©rations ({iterations}) sans √™tre compl√®tement approuv√©. Des am√©liorations suppl√©mentaires sont n√©cessaires.\n")
        
        f.write("\n## Instructions d'ex√©cution\n\n")
        
        # D√©tecter automatiquement le type de projet et g√©n√©rer des instructions d'ex√©cution
        python_files = [f for f in artifacts if f.endswith('.py') and f.startswith('code/')]
        js_files = [f for f in artifacts if f.endswith('.js') and f.startswith('code/')]
        html_files = [f for f in artifacts if f.endswith('.html') and f.startswith('code/')]
        
        if python_files:
            main_file = next((f for f in python_files if 'main' in f or 'app' in f), python_files[0])
            rel_path = os.path.join('artifacts', main_file)
            f.write(f"Pour ex√©cuter ce projet Python :\n\n")
            f.write(f"```bash\n")
            f.write(f"cd {project_name}\n")
            f.write(f"python {rel_path}\n")
            f.write(f"```\n\n")
        
        if js_files and html_files:
            f.write(f"Pour ex√©cuter ce projet web :\n\n")
            f.write(f"Ouvrez le fichier `artifacts/code/{next((f for f in html_files), '')}` dans votre navigateur.\n\n")

if __name__ == "__main__":
    # Ex√©cution directe pour les tests
    exec_crew()